---
layout: post
title:  "1. 나의 첫 머신러닝 & 2. 데이터 다루기"
date:   2024-07-29 16:42:30 +09:00
categories: ['ML기초세션']
---

## 1. 인공지능과 머신러닝, 딥러닝

**인공지능**은 사실 굉장히 오래된 개념이다.   
지금의 인공지능의 초석을 이루는 퍼셉트론 개념만 해도 1957년에 등장했다.

물론 기계가 혼자 생각하고 행동한다 등의 조금 더 원초적인 생각의 기원을 따라간다면 그 역사는 더 길어질 것이다. 아무튼 우리가 주목해야 할 부분은 **인공지능은 원래 존재했고, 시대가 따라오지 못했다**라는 것이다.

여기서 시대가 따라오지 못했다는 것은 당시에는 컴퓨팅 기술의 한계, 데이터의 양적 부족 등으로 인공지능 기술을 발전시키는 것에 한계가 있었다는 것이다.

### 1-1.  머신러닝
그렇다면 **머신러닝**은 무엇인가?   
머신러닝은 알고리즘을 통해 기계가 데이터를 학습한다는 것으로, 인공지능의 하위분야이다.
통계와 수학이 매우 중요한 분야이고, 대표 라이브러리로는 **scikit-learn**이 있다. 

정말 많은 알고리즘들이 존재하고, 계속 만들어지고 있으며, 이에 따라 다양한 라이브러리가 존재한다.

### 1-2. 딥러닝
마지막으로, **딥러닝**은 인공신경망(ANN) 기반의 알고리즘을 통칭하는 말이다. 위에 잠깐 언급한 퍼셉트론을 기반으로 이루어진 인공신경망의 Layer를 여러개 통과하면서 역전파 알고리즘을 통해 가중치를 갱신해가며 최적의 결과를 만들어나가는 방식이다. 

관련 라이브러리로는 TensorFlow, PyTorch 등이 존재한다.

## 2. 코랩과 주피터 노트북

이미 사용하고 있으므로 짧게 언급하고 넘어가면,

**코랩**은 구글에서 제공하는 웹 기반의 개발환경이다. 코랩의 장점은 구글드라이브와 연동해 파일들을 관리할 수 있다는 점과 **구글 가상 서버에서 컴퓨팅**이 이루어져 로컬 환경의 성능과 무관하게 연산량이 많거나 큰 데이터를 다룰 수 있다는 점이다. 쉽게 말하면 내 컴퓨터가 좋지 않아도 구글의 서버를 빌려서 높은 수준의 작업이 가능하다는 것이다.

그리고 이 코랩 환경은 주피터 노트북 기반이다.

이 주피터 노트북과 코랩은 셀 단위로 실행이 가능해서 단계별로 실행이 필요할때, 그리고 마크다운을 활용한 텍스트 셀을 삽입 가능하므로 상세한 설명을 제공해야 할 때 유용하다.

## 3. K-최근접 이웃 알고리즘

주요 개념을 중심으로 정리하였다.

머신러닝에서 여러 종류 중 하나를 구별하는 것을 **분류(Classification)** 이라고 한다.   
그리고 만약 이때 클래스가 두개라면 이는 **이진분류(Binary Classification)** 문제가 된다.

이 분류의 과정에서는 특성, 칼럼, Feature 등 여러 방식으로 불리는 변수를 활용한다.
즉 이 특징이 되는 독립변수들로 클래스라는 종속변수를 유추해내는 것이다.

우리가 이전에 배웠던 산점도, 그래프를 그리는 데는 **matplotlib**이 주로 활용된다.
우리가 산점도를 그린다면 어떤 형태의 그래프가 나올텐데, 만약 이 모양이 일직선에 가까울 경우 우리는 **선형적(Linear)** 이라고 말한다.

![alt text](/assets/images/2-image.png)

여기선 빙어(주황)와 도미(파랑)이 각각 선형성을 보여준다고 말할 수 있다.

이후 K-NN을 이용해 교재에서는 분류를 진행했다.
K-NN(k-최근접 이웃 알고리즘)은 분류 시에 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용하는 알고리즘이다. 이때 직선거리 연산이 요구된다.

그렇기 때문에 데이터가 매우 많을 경우, 메모리가 많이 필요하고 직선거리 연산을 위한 계산이 많아져 곤란해진다.

```python
kn49 = KNeighborsClassifier(n_neighbors=49)
```


KNeighborsClassifier는 기본적으로 주변 5개를 기준으로 판단하지만, 위와 같이 파라미터 조정을 통해 원하는 방향으로 변형이 가능하다.

- fit() : 모델에 데이터를 학습시킴 
- score() : Accuracy 기반의 점수를 나타냄  
- predict() : 특정 데이터를 넣었을 때 예측값을 반환함

## 4. 훈련 세트와 테스트 세트

### 4-1. 지도 학습과 비지도 학습

머신러닝 알고리즘은 크게 지도 학습과 비지도 학습으로 나눌 수 있다.   
**지도 학습**은 이미 무엇을 해야 할지 정해져 있다. 몇 개의 클래스로, 어떤 클래스로 분류가 이루어 져야하는지, 타겟이 정해져 있는 것이 지도 학습이다. **비지도 학습**은 반면에 그러한 타겟이 정해져 있지 않는 것이다. 

![alt text](/assets/images/2-image-1.png)

이는 가장 유명한 지도 학습인 분류 문제와 비지도 학습인 클러스터링의 차이이다. 위의 그림이 두 차이점을 명확히 보여주는 것 같다. 쉽게 비유해 말하면 분류는 나눠진 후, 이쪽은 A, 이쪽은 B라고 말할 수 있지만 클러스터링은 군집으로 묶은 다음, 이 군집끼리 비슷하다 외에 이것이 무엇인지를 이야기 할 수 없다. 애초에 무엇인지 정한 적이 없기 때문이다.

우리가 위에서 본 빙어, 도미 K-NN 분류는 무엇이 빙어이고 도미인지를 분류하는 문제였으므로, 지도 학습이라고 할 수 있다.

### 4-2. 훈련 세트와 테스트 세트

모델을 만들고 평가하는 과정은 다음과 같다.
-  **훈련 데이터**를 통해 모델을 학습시킨다.
- **테스트 데이터**를 통해 모델을 평가한다.   

그리고 우리는 이때, 훈련 데이터와 테스트 데이터를 다른 데이터로 준비해야한다.   
숙제로 이미 푼 문제를 똑같이 학교 시험에 내는 것은 의미가 없기 때문이다. 모델의 진짜 실력을 테스트하지 못하기 때문이다.

**그렇기 때문에 두 데이터를 나누는 것은 중요하다**

조금 다른 이야기이지만, 우리는 종종 숙제 문제에 나온 것만 열심히 공부해서 나머지 문제는 전혀 풀지 못하는 문제가 생긴다. 데이터 학습 과정에서도 이와 같은 문제가 생긴다. 이를 오버피팅이라고 한다.

아래는 교재의 실습과정에서 나온 여러 개념을 정리한 것이다.
- 샘플  : 개별의 하나하나의 데이터
- 인덱스 : 전체 데이터 배열에서의 위치
- 슬라이싱 : 특정 범위의 데이터들을 선택(Slicing)하는 것
```python
train_input = fish_data[:35]
train_target = fish_target[:35]
test_input = fish_data[35:]
test_target = fish_target[35:]
```
만약 위와 같이 데이터를 슬라이싱 한다면, 35번째 데이터를 기준으로 훈련 데이터와 테스트 데이터를 분리한 것이다.

참고로, 특정 데이터를 입력하면 자동으로 설정된 비율에 따라 나누어주는 메서드도 존재한다.   
예시로 scikit-learn의 train_test_split이 그러하다. 보통 이럴 때 훈련:테스트 = 7~80 : 3~20 의 비율로 나눈다.

**위와 같은 슬라이싱으로 만약에 빙어, 도미 데이터를 split 한다면 큰 문제가 생긴다.** 

테스트 데이터에 빙어만 들어가기 때문이다. 왜냐하면 원 데이터의 앞의 35개는 도미만 들어있기 때문이다! 이렇게 골고루 섞여서 들어가야 하는데 그렇지 못하는 문제를 **샘플링 편향**이라고 한다. train_test_split은 무작위로 데이터의 순서를 섞은 후 split하기에 단순 슬라이싱 보다는 이런 편향에서 안전하다.

```python
np.random.seed(42)
index = np.arange(49)
np.random.shuffle(index)
```

교재에서는 넘파이 난수 생성을 통해 인덱스를 섞어주었다.
이렇게 할 경우, 슬라이싱을 통해 split 했을 때와 달리 샘플링 편향없이 학습이 이루어지고, 올바르게 모델을 학습시켰다고 말할 수 있다. 

## 5. 데이터 전처리

### 5-1. 넘파이
넘파이의 column_stack()은 리스트를 연결해 튜플로 반환한다.
```python
np.column_stack(([1,2,3],[4,5,6]))
#array([[1,4],
#       [2,5],
#       [3,6]])
```
**튜플**은 리스트와 비슷하지만 **가변적이지 않다**. 이런 특성때문에 매개변수 값으로 많이 사용한다.   
참고로, 넘파이 배열을 출력할 경우, 리스트처럼 한 줄로 길게 출력되지 않고 행과 열을 맞춰 출력된다!

+np.ones() 와 np.zeros()를 이용하면 1과 0으로 채운 배열을 쉽게 만들 수 있다.

np.concatenate()를 사용할 경우, 하나의 배열로 연결할 수 있다. column_stack과 배열을 결합한다는 점에서 동일하지만 결과물이 다르다.

![alt text](/assets/images/2-image-2.png)

### 5-2. 사이킷런을 통한 Split

아까 위에서 먼저 언급한 **train_test_split()**에 대한 부연설명이다.   
아래는 해당 train_test_split이 가지는 파라미터이다.
- random_state : 셔플을 위한 랜덤 시드를 지정해 준다. 
- stratify : 타겟 데이터를 전달하면 각 데이터 세트에 클래스 비율을 고려하여 split한다.
**stratify**는 훈련 데이터가 작거나 특정 클래스의 샘플 개수가 적을 때 유리하다. 

### 5-3. 스케일 전처리

![alt text](/assets/images/2-image-3.png)

위의 산점도를 확인하면, 삼각형으로 표시된 샘플이 보인다. 이 샘플은 현재 도미에 가깝지만 현재 빙어라고 판단된다. 분명 오른쪽 도미군집에 가까운데 이런일이 왜 일어나는 것 일까?

kneighbors() 메서드를 이용해 가까운 이웃을 찾을 수 있다.
```python
distances, indexes = kn.neighbors([25,150])
```
이를 이용해 근접 이웃들의 산점도와 거리를 확인하면 아래와 같다.

![alt text](/assets/images/2-image-4.png)

```python
print(distances)
#[[92.00086956 130.48375378 130.73859415 138.32150953 138.39320793]]
```
이는 직관적으로 이상한데, 가장 가까운 마름모까지 거리와 두번째 가까운 마름모까지 거리의 차이는 몇배는 나보이는데, 이게 92, 130 이라는 것이다.

y축의 스케일이 x축의 스케일보다 크기 때문에 이런 일이 일어나는 것이다.
이러한 문제를 해결하기 위해 **표준점수**를 활용한 전처리를 진행한다.

표준점수는, 각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다.
```python
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)
train_scaled = (train_input - mean)/std
```
참고로 train_input에 mean을 빼준다는 부분을 보면, 뭔가 저게 어떻게 말이되지? 라고 생각할 수 있는데 넘파이는 이를 우리가 생각하는 데로 처리해준다. **브로드캐스팅**이라고 말한다.

![alt text](/assets/images/2-image-5.png)

위의 과정을 통해 훈련 데이터, 테스트 데이터와 샘플 데이터를 수정한 후에, 즉 스케일 문제를 해결한 후에 모델을 다시 훈련시킨 후 예측을 하면 올바르게 작동하는 것을 확인할 수 있다.